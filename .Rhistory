```
```{r rf_plot}
rfFit
plot(rfFit, ylim = c(0.9, 1))
```
```{r rf_imp, echo = F}
imp <- varImp(rfFit)$importance
imp$max <- apply(imp, 1, max)
imp <- imp[order(imp$max, decreasing = T), ]
```
The random forests algorithm generated a very accurate model with __accuracy close to 1__. Compared to boosting model, this model generally has better performance in terms of accuracy as we see from the plots.
### Final model and prediction
* Comparing model accuracy of the two models generated, random forests and boosting, random forests model has overall better accuracy. So, I'll use this model for prediction.
* The final random forests model contains 500 trees with 40 variables tried at each split. The five most important predictors in this model are `r rownames(imp)[1:5]`.
* Estimated __out of sample error rate__ for the random forests model is __0.04%__ as reported by the final model.
* Predict the test set and output results for automatic grader.
```{r prediction, message = F}
# final model
rfFit$finalModel
# prediction
(prediction <- as.character(predict(rfFit, testing)))
```
```{r results, eval = F}
# write prediction files
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("./prediction/problem_id_", i, ".txt")
write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
}
}
pml_write_files(prediction)
```
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
### Load data
set.seed(12345)
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
install.packages("rpart")
install.packages("rpart.plot")
install.packages("RColorBrewer")
install.packages("rattle")
install.packages("knitr")
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(knitr)
library(ggplot2)
library(caret)
library(randomForest)
setwd("C:/Users/rcbull/Downloads")
load("project.Rdata")
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
myTraining <- myTraining[c(-1)]
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(knitr)
library(ggplot2)
library(caret)
library(randomForest)
setwd("C:/Users/rcbull/Downloads")
load("project.Rdata")
```
### Load data
set.seed(12345)
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
myTraining <- myTraining[c(-1)]
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
trainingV3 <- trainingV3[ , -j]
}
}
}
}
# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining
dim(myTesting)
dim(testing)
for (i in 1:length(testing) ) {
for(j in 1:length(myTraining)) {
if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
class(testing[j]) <- class(myTraining[i])
}
}
}
# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
plot(modFitB1)
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
trControl = fitControl,
verbose = FALSE)
### Load data
set.seed(12345)
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
myTraining <- myTraining[c(-1)]
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
trainingV3 <- trainingV3[ , -j]
}
}
}
}
# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining
dim(myTesting)
dim(testing)
for (i in 1:length(testing) ) {
for(j in 1:length(myTraining)) {
if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
class(testing[j]) <- class(myTraining[i])
}
}
}
# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
plot(modFitB1)
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
trControl = fitControl,
verbose = FALSE)
2
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
myTraining <- myTraining[c(-1)]
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
trainingV3 <- trainingV3[ , -j]
}
}
}
}
# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining
dim(myTesting)
dim(testing)
setwd("C:/Users/rcbull/Downloads/PracticalMachineLearning")
library(data.table)
setInternet2(TRUE)
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
D <- fread(url)
install.packages("data.table")
library(data.table)
setInternet2(TRUE)
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
D <- fread(url)
```
Load the testing data into a data table.
```{r}
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
DTest <- fread(url)
```
Which variables in the test dataset have zero `NA`s?
Use this tip: [finding columns with all missing values in r](http://stackoverflow.com/a/11330265).
Belt, arm, dumbbell, and forearm variables that do not have any missing values in the test dataset will be **predictor candidates**.
```{r}
isAnyMissing <- sapply(DTest, function (x) any(is.na(x) | x == ""))
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predCandidates <- names(isAnyMissing)[isPredictor]
predCandidates
```
Subset the primary dataset to include only the **predictor candidates** and the outcome variable, `classe`.
```{r}
varToInclude <- c("classe", predCandidates)
D <- D[, varToInclude, with=FALSE]
dim(D)
names(D)
```
Make `classe` into a factor.
```{r}
D <- D[, classe := factor(D[, classe])]
D[, .N, classe]
```
Split the dataset into a 60% training and 40% probing dataset.
```{r}
library(caret)
seed <- as.numeric(as.Date("2014-10-26"))
set.seed(seed)
inTrain <- createDataPartition(D$classe, p=0.6)
DTrain <- D[inTrain[[1]]]
DProbe <- D[-inTrain[[1]]]
```
Preprocess the prediction variables by centering and scaling.
```{r}
X <- DTrain[, predCandidates, with=FALSE]
preProc <- preProcess(X)
preProc
XCS <- predict(preProc, X)
DTrainCS <- data.table(data.frame(classe = DTrain[, classe], XCS))
```
Apply the centering and scaling to the probing dataset.
```{r}
X <- DProbe[, predCandidates, with=FALSE]
XCS <- predict(preProc, X)
DProbeCS <- data.table(data.frame(classe = DProbe[, classe], XCS))
```
Check for near zero variance.
```{r}
nzv <- nearZeroVar(DTrainCS, saveMetrics=TRUE)
if (any(nzv$nzv)) nzv else message("No variables with near zero variance")
```
Examine groups of prediction variables.
```{r histGroup}
histGroup <- function (data, regex) {
col <- grep(regex, names(data))
col <- c(col, which(names(data) == "classe"))
library(reshape2)
n <- nrow(data)
DMelted <- melt(data[, col, with=FALSE][, rownum := seq(1, n)], id.vars=c("rownum", "classe"))
library(ggplot2)
ggplot(DMelted, aes(x=classe, y=value)) +
geom_violin(aes(color=classe, fill=classe), alpha=1/2) +
#     geom_jitter(aes(color=classe, fill=classe), alpha=1/10) +
#     geom_smooth(aes(group=1), method="gam", color="black", alpha=1/2, size=2) +
facet_wrap(~ variable, scale="free_y") +
scale_color_brewer(palette="Spectral") +
scale_fill_brewer(palette="Spectral") +
labs(x="", y="") +
theme(legend.position="none")
}
histGroup(DTrainCS, "belt")
histGroup(DTrainCS, "[^(fore)]arm")
histGroup(DTrainCS, "dumbbell")
histGroup(DTrainCS, "forearm")
```
# Train a prediction model
Using random forest, the out of sample error should be small.
The error will be estimated using the 40% probing sample.
I would be quite happy with an error estimate of 3% or less.
Set up the parallel clusters.
```{r}
library(parallel)
library(doParallel)
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
```
Set the control parameters.
```{r}
ctrl <- trainControl(classProbs=TRUE,
savePredictions=TRUE,
allowParallel=TRUE)
```
Fit model over the tuning parameters.
```{r}
method <- "rf"
system.time(trainingModel <- train(classe ~ ., data=DTrainCS, method=method))
```
Stop the clusters.
```{r}
stopCluster(cl)
```
## Evaluate the model on the training dataset
```{r}
trainingModel
hat <- predict(trainingModel, DTrainCS)
confusionMatrix(hat, DTrain[, classe])
```
## Evaluate the model on the probing dataset
```{r}
hat <- predict(trainingModel, DProbeCS)
confusionMatrix(hat, DProbeCS[, classe])
```
## Display the final model
```{r finalModel}
varImp(trainingModel)
trainingModel$finalModel
```
**The estimated error rate is less than 1%.**
Save training model object for later.
```{r}
save(trainingModel, file="trainingModel.RData")
```
# Predict on the test data
Load the training model.
```{r}
load(file="trainingModel.RData", verbose=TRUE)
```
Get predictions and evaluate.
```{r}
DTestCS <- predict(preProc, DTest[, predCandidates, with=FALSE])
hat <- predict(trainingModel, DTestCS)
DTest <- cbind(hat , DTest)
subset(DTest, select=names(DTest)[grep("belt|[^(fore)]arm|dumbbell|forearm", names(DTest), invert=TRUE)])
```
## Submission to Coursera
Write submission files to `predictionAssignment_files/answers`.
```{r}
pml_write_files = function(x){
n = length(x)
path <- "predictionAssignment_files/answers"
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(hat)
```
setwd("C:/Users/rcbull/Downloads/PracticalMachineLearning")
---
title: "Practical Machine Learning Course Project Report"
author: "gloriali"
date: "June 20, 2014"
output:
html_document:
toc: yes
---
```{r prep, echo = F, message = F}
library(ggplot2)
library(caret)
library(randomForest)
setwd("C:/Users/rcbull/Downloads/PracticalMachineLearning")
load("project.Rdata")
```
### Load data
* Load data.
* Remove near zero covariates and those with more than 80% missing values since these variables will not provide much power for prediction.
* Calculate correlations between each remaining feature to the response, `classe`. Use `spearman` rank based correlation because `classe` is a factor.
* Plot the two features that have highest correlation with `classe` and color with `classe` to see if we can separate response based on these features.
```{r data, eval = F}
# load data
training <- read.csv("./data/pml-training.csv", row.names = 1)
testing <- read.csv("./data/pml-testing.csv", row.names = 1)
# remove near zero covariates
nsv <- nearZeroVar(training, saveMetrics = T)
training <- training[, !nsv$nzv]
# remove variables with more than 80% missing values
nav <- sapply(colnames(training), function(x) if(sum(is.na(training[, x])) > 0.8*nrow(training)){return(T)}else{return(F)})
training <- training[, !nav]
# calculate correlations
cor <- abs(sapply(colnames(training[, -ncol(training)]), function(x) cor(as.numeric(training[, x]), as.numeric(training$classe), method = "spearman")))
```
```{r data_plot}
# plot predictors
summary(cor)
plot(training[, names(which.max(cor))], training[, names(which.max(cor[-which.max(cor)]))], col = training$classe, pch = 19, cex = 0.1, xlab = names(which.max(cor)), ylab = names(which.max(cor[-which.max(cor)])))
```
The training set has __`r nrow(training)`__ samples and __`r ncol(training) - 1`__ potential predictors after filtering.
There doesn't seem to be any strong predictors that correlates with `classe` well, so linear regression model is probably not suitable in this case. Boosting and random forests algorithms may generate more robust predictions for our data.
### Boosting model
* Fit model with boosting algorithm and 10-fold cross validation to predict `classe` with all other predictors.
* Plot accuracy of this model on the scale `[0.9, 1]`.
```{r boost, eval = F}
set.seed(123)
boostFit <- train(classe ~ ., method = "gbm", data = training, verbose = F, trControl = trainControl(method = "cv", number = 10))
```
```{r boost_plot}
boostFit
plot(boostFit, ylim = c(0.9, 1))
```
The boosting algorithm generated a good model with __accuracy = 0.997__.
### Random forests model
* Fit model with random forests algorithm and 10-fold cross validation to predict `classe` with all other predictors.
* Plot accuracy of the model on the __same scale__ as boosting model.
```{r rf, eval = F}
set.seed(123)
rfFit <- train(classe ~ ., method = "rf", data = training, importance = T, trControl = trainControl(method = "cv", number = 10))
```
```{r rf_plot}
rfFit
plot(rfFit, ylim = c(0.9, 1))
```
```{r rf_imp, echo = F}
imp <- varImp(rfFit)$importance
imp$max <- apply(imp, 1, max)
imp <- imp[order(imp$max, decreasing = T), ]
```
The random forests algorithm generated a very accurate model with __accuracy close to 1__. Compared to boosting model, this model generally has better performance in terms of accuracy as we see from the plots.
### Final model and prediction
* Comparing model accuracy of the two models generated, random forests and boosting, random forests model has overall better accuracy. So, I'll use this model for prediction.
* The final random forests model contains 500 trees with 40 variables tried at each split. The five most important predictors in this model are `r rownames(imp)[1:5]`.
* Estimated __out of sample error rate__ for the random forests model is __0.04%__ as reported by the final model.
* Predict the test set and output results for automatic grader.
```{r prediction, message = F}
# final model
rfFit$finalModel
# prediction
(prediction <- as.character(predict(rfFit, testing)))
```
```{r results, eval = F}
# write prediction files
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("./prediction/problem_id_", i, ".txt")
write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
}
}
pml_write_files(prediction)
```
instal.packages("margin")
install.packages("margin")
setwd("C:/Users/rcbull/Downloads/PracticalMachineLearning")
setwd("C:/Users/rcbull/Downloads/PracticalMachineLearning")
install.packages("corrplot")
install.packages("Rtsne")
install.packages("xgboost")
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("e701")
install.packages("e1071")
install.packages("ckmeans")
install.packages("ckmeans")
install.packages("ckmeans")
install.packages("Ckmeans.1d.dp")
